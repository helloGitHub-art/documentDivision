import cv2
import numpy as np
import os
import torch
import time  # æ–°å¢ï¼šå¯¼å…¥æ—¶é—´æ¨¡å—
from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
from pathlib import Path  # æ–¹ä¾¿è·¯å¾„å¤„ç†

def sam_generate_mask_canny_and_annotate(img_path, sam_model_path, output_dir, 
                                         model_type="vit_b", canny_low=50, canny_high=150, 
                                         show_process=False, save_all=True):
    """
    å•å¼ å›¾ç‰‡å¤„ç†ï¼šSAMç”Ÿæˆæ©ç  â†’ Cannyè¾¹ç¼˜æ£€æµ‹ â†’ åŸå›¾æ ‡æ³¨ï¼ˆå¼ºåˆ¶GPUè¿è¡Œï¼‰
    :param img_path: å•å¼ å›¾ç‰‡ç»å¯¹è·¯å¾„
    :param sam_model_path: SAMæ¨¡å‹æƒé‡è·¯å¾„ï¼ˆ.pthï¼‰
    :param output_dir: ç»“æœä¿å­˜çš„æ ¹ç›®å½•ï¼ˆç»Ÿä¸€æŒ‡å®šï¼‰
    :param model_type: SAMæ¨¡å‹ç±»å‹ï¼ˆvit_b/vit_l/vit_hï¼‰
    :param canny_low: Cannyä½é˜ˆå€¼
    :param canny_high: Cannyé«˜é˜ˆå€¼
    :param show_process: æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–çª—å£ï¼ˆæ‰¹é‡å¤„ç†å»ºè®®å…³é—­ï¼‰
    :param save_all: æ˜¯å¦ä¿å­˜æ‰€æœ‰ç»“æœå›¾ï¼ˆå«æ©ç ï¼‰
    :return: (å¤„ç†æˆåŠŸæ ‡å¿—, å¤„ç†è€—æ—¶ç§’æ•°) â†’ ä¿®æ”¹ï¼šè¿”å›è€—æ—¶
    """
    # æ–°å¢ï¼šè®°å½•å•å¼ å›¾ç‰‡å¤„ç†å¼€å§‹æ—¶é—´
    start_time = time.time()
    try:
        # ========== GPUè¿è¡Œæ ¸å¿ƒé…ç½® ==========
        if not torch.cuda.is_available():
            raise RuntimeError("æœªæ£€æµ‹åˆ°CUDA GPUï¼è¯·ç¡®è®¤æ˜¾å¡é©±åŠ¨/å®‰è£…CUDAç‰ˆæœ¬/PyTorchæ˜¯å¦æ”¯æŒGPU")
        
        device = torch.device("cuda:0")
        torch.cuda.set_device(device)

        # ========== å›¾ç‰‡é¢„å¤„ç† ==========
        img = cv2.imread(img_path)
        if img is None:
            print(f"âš ï¸ è·³è¿‡ï¼šæœªæ‰¾åˆ°å›¾ç‰‡ {img_path}")
            return False, time.time() - start_time  # ä¿®æ”¹ï¼šè¿”å›è€—æ—¶
        img_original = img.copy()
        h_ori, w_ori = img_original.shape[:2]
        
        # ç¼©æ”¾å›¾ç‰‡ï¼ˆGPUè¶³å¤Ÿæ—¶æ”¾å¤§åˆ°1000åƒç´ ï¼‰
        # scale = 1000 / max(h_ori, w_ori)
        #800å½±å“image3ï¼Œ1000å½±å“image4
        scale = 800 / max(h_ori, w_ori)

        img_resize = cv2.resize(img, (int(w_ori*scale), int(h_ori*scale)))
        img_rgb = cv2.cvtColor(img_resize, cv2.COLOR_BGR2RGB)

        # ========== é‡æ„ä¿å­˜è·¯å¾„ï¼ˆç»Ÿä¸€åˆ°æŒ‡å®šresultç›®å½•ï¼‰ ==========
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        # è·å–å›¾ç‰‡æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰
        img_name = os.path.splitext(os.path.basename(img_path))[0]
        # ç»Ÿä¸€ä¿å­˜è·¯å¾„ï¼ˆä¸å†ä¾èµ–åŸå›¾ç‰‡ç›®å½•ï¼‰
        save_path_mask = os.path.join(output_dir, f"{img_name}_sam_mask.png")
        save_path_canny = os.path.join(output_dir, f"{img_name}_sam_canny.png")
        save_path_annotated = os.path.join(output_dir, f"{img_name}_sam_annotated.png")

        # ========== åŠ è½½SAMæ¨¡å‹ï¼ˆå¼ºåˆ¶GPUï¼‰ ==========
        torch.cuda.empty_cache()
        sam = sam_model_registry[model_type](checkpoint=sam_model_path)
        sam.to(device=device)
        sam.eval()

        # ========== SAMæ©ç ç”Ÿæˆå™¨ï¼ˆGPUä¼˜åŒ–å‚æ•°ï¼‰ ==========
        mask_generator = SamAutomaticMaskGenerator(
            model=sam,
            points_per_side=32,
            pred_iou_thresh=0.9,
            stability_score_thresh=0.9,
            crop_n_layers=1,
            crop_n_points_downscale_factor=2,
            min_mask_region_area=1000,
            output_mode="binary_mask"
        )

        # ========== ç”Ÿæˆæ©ç  ==========
        masks = mask_generator.generate(img_rgb)
        if not masks:
            print(f"âš ï¸ è·³è¿‡ï¼š{img_path} SAMæœªç”Ÿæˆä»»ä½•æ©ç ")
            return False, time.time() - start_time  # ä¿®æ”¹ï¼šè¿”å›è€—æ—¶
        masks = sorted(masks, key=lambda x: x['area'], reverse=True)
        best_mask = masks[0]['segmentation']
        mask_binary = (best_mask.astype(np.uint8)) * 255

        # ========== Cannyè¾¹ç¼˜æ£€æµ‹ ==========
        mask_blurred = cv2.GaussianBlur(mask_binary, (5, 5), 0)
        canny_edges = cv2.Canny(mask_blurred, canny_low, canny_high)
        kernel = np.ones((3, 3), np.uint8)
        canny_edges = cv2.morphologyEx(canny_edges, cv2.MORPH_CLOSE, kernel)

        # ========== é¡¶ç‚¹æ‹Ÿåˆ+åŸå›¾æ ‡æ³¨ ==========
        contours, _ = cv2.findContours(canny_edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours:
            print(f"âš ï¸ è·³è¿‡ï¼š{img_path} ä»Cannyè¾¹ç¼˜å›¾æœªæå–åˆ°è½®å»“")
            return False, time.time() - start_time  # ä¿®æ”¹ï¼šè¿”å›è€—æ—¶
        max_contour = max(contours, key=cv2.contourArea)

        # å®¹é”™æ‹Ÿåˆé¡¶ç‚¹
        perimeter = cv2.arcLength(max_contour, True)
        doc_corners = None
        for epsilon_ratio in [0.02, 0.015, 0.025, 0.01, 0.03, 0.04]:
            approx = cv2.approxPolyDP(max_contour, epsilon_ratio * perimeter, True)
            if len(approx) == 4:
                doc_corners = approx.reshape(4, 2)
                break
        
        if doc_corners is None:
            print(f"âš ï¸ {img_path}ï¼šæœªæ‹Ÿåˆå‡º4ä¸ªé¡¶ç‚¹ï¼Œä½¿ç”¨å‡¸åŒ…è¿‘ä¼¼")
            hull = cv2.convexHull(max_contour)
            hull_perimeter = cv2.arcLength(hull, True)
            for epsilon_ratio in [0.02, 0.03, 0.04]:
                approx = cv2.approxPolyDP(hull, epsilon_ratio * hull_perimeter, True)
                if len(approx) == 4:
                    doc_corners = approx.reshape(4, 2)
                    break
        
        if doc_corners is None:
            print(f"âš ï¸ {img_path}ï¼šå‡¸åŒ…è¿‘ä¼¼å¤±è´¥ï¼Œä½¿ç”¨è½®å»“æå€¼ç‚¹")
            leftmost = tuple(max_contour[max_contour[:, :, 0].argmin()][0])
            rightmost = tuple(max_contour[max_contour[:, :, 0].argmax()][0])
            topmost = tuple(max_contour[max_contour[:, :, 1].argmin()][0])
            bottommost = tuple(max_contour[max_contour[:, :, 1].argmax()][0])
            doc_corners = np.array([leftmost, rightmost, bottommost, topmost], dtype=np.int32)

        # è¿˜åŸåæ ‡+æ ‡æ³¨
        doc_corners_ori = (doc_corners / scale).astype(np.int32)
        img_annotated = img_original.copy()
        cv2.drawContours(img_annotated, [doc_corners_ori], -1, (0, 255, 0), 2)
        for i, (x, y) in enumerate(doc_corners_ori):
            cv2.circle(img_annotated, (x, y), 6, (0, 0, 255), -1)
            cv2.putText(img_annotated, f"({x},{y})", (x-30, y-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)

        # ========== ä¿å­˜ç»“æœï¼ˆå¼ºåˆ¶ä¿ç•™æ©ç ï¼‰ ==========
        if save_all:
            cv2.imwrite(save_path_mask, mask_binary)  # å¿…ä¿å­˜æ©ç 
            cv2.imwrite(save_path_canny, canny_edges)
            cv2.imwrite(save_path_annotated, img_annotated)
            # æ–°å¢ï¼šæ‰“å°è€—æ—¶ï¼ˆä¿ç•™2ä½å°æ•°ï¼Œæ˜¾ç¤ºæ¯«ç§’çº§ï¼‰
            elapsed_time = time.time() - start_time
            print(f"âœ… å·²å¤„ç†ï¼š{img_name} â†’ ç»“æœä¿å­˜è‡³ {output_dir} | è€—æ—¶ï¼š{elapsed_time:.2f} ç§’")

        # ========== å¯è§†åŒ–ï¼ˆæ‰¹é‡å¤„ç†å»ºè®®å…³é—­ï¼‰ ==========
        if show_process:
            cv2.imshow("1-SAMæ©ç ï¼ˆä¿ç•™ï¼‰", mask_binary)
            cv2.imshow("2-Cannyè¾¹ç¼˜", canny_edges)
            cv2.imshow("3-æ ‡æ³¨åçš„åŸå›¾", img_annotated)
            cv2.waitKey(0)
            cv2.destroyAllWindows()

        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        elapsed_time = time.time() - start_time
        return True, elapsed_time  # ä¿®æ”¹ï¼šè¿”å›æˆåŠŸæ ‡å¿—+è€—æ—¶

    except Exception as e:
        elapsed_time = time.time() - start_time  # æ–°å¢ï¼šå¼‚å¸¸æ—¶ä¹Ÿè®°å½•è€—æ—¶
        print(f"âŒ å¤„ç†å¤±è´¥ï¼š{os.path.basename(img_path)} â†’ {str(e)} | è€—æ—¶ï¼š{elapsed_time:.2f} ç§’")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return False, elapsed_time  # ä¿®æ”¹ï¼šè¿”å›å¤±è´¥æ ‡å¿—+è€—æ—¶

def batch_process_images(input_dir, output_dir, sam_model_path, model_type="vit_b"):
    """
    æ‰¹é‡å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡
    :param input_dir: è¾“å…¥å›¾ç‰‡ç›®å½•ï¼ˆH:\PythonProject\vsPythonPro\docDiv\data\test\test-imageï¼‰
    :param output_dir: è¾“å‡ºç»“æœç›®å½•ï¼ˆH:\PythonProject\vsPythonPro\docDiv\data\test\resultï¼‰
    :param sam_model_path: SAMæ¨¡å‹æƒé‡è·¯å¾„
    :param model_type: SAMæ¨¡å‹ç±»å‹
    """
    # æ–°å¢ï¼šè®°å½•æ‰¹é‡å¤„ç†æ€»å¼€å§‹æ—¶é—´
    batch_start_time = time.time()
    
    # 1. æ ¡éªŒè¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_dir):
        raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨ï¼š{input_dir}")
    
    # 2. æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
    supported_formats = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.tif')
    
    # 3. è·å–ç›®å½•ä¸‹æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    img_files = [
        os.path.join(input_dir, f) 
        for f in os.listdir(input_dir)
        if f.lower().endswith(supported_formats)
    ]
    
    if not img_files:
        print(f"âš ï¸ è¾“å…¥ç›®å½• {input_dir} ä¸‹æœªæ‰¾åˆ°æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶")
        return
    
    # 4. æ‰¹é‡å¤„ç†
    total = len(img_files)
    success_count = 0
    total_elapsed_time = 0.0  # æ–°å¢ï¼šç»Ÿè®¡æ‰€æœ‰å›¾ç‰‡æ€»è€—æ—¶
    print(f"\nğŸ“Œ å¼€å§‹æ‰¹é‡å¤„ç†ï¼šå…± {total} å¼ å›¾ç‰‡")
    print(f"ğŸ“Œ è¾“å…¥ç›®å½•ï¼š{input_dir}")
    print(f"ğŸ“Œ è¾“å‡ºç›®å½•ï¼š{output_dir}\n")

    for idx, img_path in enumerate(img_files, 1):
        print(f"[{idx}/{total}] æ­£åœ¨å¤„ç†ï¼š{os.path.basename(img_path)}")
        # ä¿®æ”¹ï¼šæ¥æ”¶å¤„ç†ç»“æœå’Œè€—æ—¶
        success, elapsed = sam_generate_mask_canny_and_annotate(
            img_path=img_path,
            sam_model_path=sam_model_path,
            output_dir=output_dir,
            model_type=model_type
        )
        if success:
            success_count += 1
        total_elapsed_time += elapsed  # æ–°å¢ï¼šç´¯åŠ æ€»è€—æ—¶

    # 5. è¾“å‡ºå¤„ç†æ€»ç»“
    batch_total_time = time.time() - batch_start_time  # æ–°å¢ï¼šæ‰¹é‡æ€»è€—æ—¶
    avg_time_per_img = total_elapsed_time / total if total > 0 else 0  # æ–°å¢ï¼šå¹³å‡å•å¼ è€—æ—¶
    
    print(f"\nğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆï¼")
    print(f"âœ… æˆåŠŸï¼š{success_count} å¼ ")
    print(f"âŒ å¤±è´¥ï¼š{total - success_count} å¼ ")
    print(f"â±ï¸  å•å¼ å¹³å‡è€—æ—¶ï¼š{avg_time_per_img:.2f} ç§’/å¼ ")  # æ–°å¢
    print(f"â±ï¸  æ‰¹é‡æ€»è€—æ—¶ï¼š{batch_total_time:.2f} ç§’")        # æ–°å¢
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³ï¼š{output_dir}")

# ------------------- æ‰¹é‡å¤„ç†è°ƒç”¨å…¥å£ -------------------
if __name__ == "__main__":
    # é…ç½®å‚æ•°ï¼ˆä»…éœ€ä¿®æ”¹è¿™3ä¸ªè·¯å¾„ï¼‰
    SAM_MODEL_PATH = r"H:\PythonProject\vsPythonPro\docDiv\sam_weights\sam_vit_b_01ec64.pth"
    INPUT_DIR = r"H:\PythonProject\vsPythonPro\docDiv\data\test\test-image"  # å¾…å¤„ç†å›¾ç‰‡ç›®å½•
    OUTPUT_DIR = r"H:\PythonProject\vsPythonPro\docDiv\data\test\result"      # ç»“æœä¿å­˜ç›®å½•

    try:
        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        batch_process_images(
            input_dir=INPUT_DIR,
            output_dir=OUTPUT_DIR,
            sam_model_path=SAM_MODEL_PATH,
            model_type="vit_b"
        )
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡å¤„ç†åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()